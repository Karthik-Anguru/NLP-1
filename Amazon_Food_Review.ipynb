{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9p8DFQwRJ/tiGP6aAmJxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthik-Anguru/NLP-1/blob/main/Amazon_Food_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmE1BwjPtqxZ"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "import pandas as pd\n",
        "\n",
        "#Initialize SageMaker seesion\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Get the SageMaker execution role\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "#S3 bucket for storing data\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "\n",
        "prefix =\"nlp-model-demo\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "df = pd.read_csv(\"Reviews.csv\")  # Ensure the dataset is available locally\n",
        "\n",
        "# Keep only required columns\n",
        "\n",
        "df = df[[\"Text\", \"Score\"]].dropna()\n",
        "\n",
        "# Convert scores into binary sentiment (1 = Positive, 0 = Negative)\n",
        "\n",
        "df[\"Sentiment\"] = df[\"Score\"].apply(lambda x: 1 if x > 3 else 0)\n",
        "\n",
        "df = df[[\"Text\", \"Sentiment\"]]\n",
        "\n",
        "# Save processed data\n",
        "\n",
        "df.to_csv(\"processed_reviews.csv\", index=False)\n",
        "\n",
        "# Upload to S3\n",
        "\n",
        "s3 = boto3.client(\"s3\")\n",
        "\n",
        "s3.upload_file(\"processed_reviews.csv\", bucket, f\"{prefix}/processed_reviews.csv\")\n",
        "\n",
        "# Store S3 path\n",
        "\n",
        "s3_train_data = f\"s3://{bucket}/{prefix}/processed_reviews.csv\"\n",
        "\n",
        "print(\"Data uploaded to:\", s3_train_data)\n"
      ],
      "metadata": {
        "id": "7_rBVpZQwSR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import argparse\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import joblib\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "metadata": {
        "id": "cW2JhYgtwT44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Argument parser for SageMaker input\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--train_data\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "    # Load dataset from the provided path\n",
        "\n",
        "    train_data_path = os.path.join(args.train_data, \"processed_reviews.csv\")\n",
        "\n",
        "    df = pd.read_csv(train_data_path)\n",
        "\n",
        "\n",
        "\n",
        "    # Split data\n",
        "\n",
        "    X = df[\"Text\"]\n",
        "\n",
        "    y = df[\"Sentiment\"]\n"
      ],
      "metadata": {
        "id": "Vm2FyTDtx0t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a text-processing pipeline\n",
        "    pipeline = Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
        "        (\"clf\", LogisticRegression())\n",
        "    ])\n",
        "    # Train model\n",
        "    pipeline.fit(X, y)\n",
        "    # Save trained model\n",
        "    model_path = os.path.join(\"/opt/ml/model\", \"model.joblib\")\n",
        "    joblib.dump(pipeline, model_path)\n",
        "    print(\"Model saved at\", model_path)\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "id": "JK58rVy-yAPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.sklearn.estimator import SKLearn\n",
        "\n",
        "\n",
        "\n",
        "# Define SageMaker SKLearn Estimator\n",
        "\n",
        "sklearn_estimator = SKLearn(\n",
        "\n",
        "    entry_point=\"train.py\",\n",
        "\n",
        "    framework_version=\"0.23-1\",\n",
        "\n",
        "    instance_type=\"ml.m5.large\",\n",
        "\n",
        "    role=role,\n",
        "\n",
        "    sagemaker_session=sagemaker_session,\n",
        "\n",
        ")\n",
        "\n",
        "# Train the model on SageMaker\n",
        "\n",
        "sklearn_estimator.fit({\"train\": s3_train_data})"
      ],
      "metadata": {
        "id": "aD_5Bsn7zNod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile inference.py\n",
        "\n",
        "import joblib\n",
        "\n",
        "import os\n",
        "\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load trained model\n",
        "\n",
        "def model_fn(model_dir):\n",
        "\n",
        "    model_path = os.path.join(model_dir, \"model.joblib\")\n",
        "\n",
        "    return joblib.load(model_path)\n",
        "\n",
        "# Parse input JSON\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "\n",
        "    if request_content_type == \"application/json\":\n",
        "\n",
        "        data = json.loads(request_body)\n",
        "\n",
        "        return pd.DataFrame(data, columns=[\"Text\"])\n",
        "\n",
        "    else:\n",
        "\n",
        "        raise ValueError(\"Unsupported content type: {}\".format(request_content_type))\n",
        "\n",
        "# Generate predictions\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "\n",
        "    return model.predict(input_data[\"Text\"]).tolist()"
      ],
      "metadata": {
        "id": "73BRvpq00lW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.sklearn.model import SKLearnModel\n",
        "\n",
        "# Get model path from training job\n",
        "\n",
        "model_data = sklearn_estimator.model_data\n",
        "\n",
        "\n",
        "\n",
        "# Create a SageMaker model\n",
        "\n",
        "sklearn_model = SKLearnModel(\n",
        "\n",
        "    model_data=model_data,\n",
        "\n",
        "    role=role,\n",
        "\n",
        "    entry_point=\"inference.py\",\n",
        "\n",
        "    framework_version=\"0.23-1\",\n",
        "\n",
        "    sagemaker_session=sagemaker_session,\n",
        "\n",
        ")\n",
        "\n",
        "# Deploy the model to a real-time endpoint\n",
        "\n",
        "predictor = sklearn_model.deploy(instance_type=\"ml.m5.large\", initial_instance_count=1)"
      ],
      "metadata": {
        "id": "F5hkqWR62a-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Sample test data\n",
        "test_data = json.dumps([\"This product is amazing!\", \"Worst product ever.\"])\n",
        "response = predictor.predict(test_data)\n",
        "print(\"Predictions:\", response)\n"
      ],
      "metadata": {
        "id": "hoiX10uy22mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.delete_endpoint()"
      ],
      "metadata": {
        "id": "XHcqxDSz3GfU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}